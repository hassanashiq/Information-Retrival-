{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semester Project : Information Retrieval System\n",
    " ### Submitted to Ma'am Naima\n",
    "- #### Submitted by : Hassan Ashiq BESE 23 C , Usman Ali Abbasi 23 C\n",
    "- ###### Link to my GitHub Repository <a href=\"https://github.com/hassanashiqasse/PCA\">Click Here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CISI.ALL File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open('IR Project/CISI.ALL') as CISI_file:\n",
    "    lines = \"\"\n",
    "    for l in CISI_file.readlines():\n",
    "        lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "    lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placing each document in CISI File in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents = 1460.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_set = {}\n",
    "doc_id = \"\"\n",
    "doc_text = \"\"\n",
    "for l in lines:\n",
    "    if l.startswith(\".I\"):\n",
    "        doc_id = l.split(\" \")[1].strip()\n",
    "    elif l.startswith(\".X\"):\n",
    "        doc_set[doc_id] = doc_text.lstrip(\" \")\n",
    "        doc_id = \"\"\n",
    "        doc_text = \"\"\n",
    "    else:\n",
    "        doc_text += l.strip()[3:] + \" \" # The first 3 characters of a line can be ignored.\n",
    "\n",
    "# Print something to see the dictionary structure, etc.\n",
    "print(f\"Number of documents = {len(doc_set)}\" + \".\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CISI Query File and placing each query in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries = 112.\n",
      "\n",
      "Query # 2 :  How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests?\n"
     ]
    }
   ],
   "source": [
    "with open('IR Project/datasets.QRY') as f:\n",
    "    lines = \"\"\n",
    "    for l in f.readlines():\n",
    "        lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "    lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "    \n",
    "qry_set = {}\n",
    "qry_id = \"\"\n",
    "for l in lines:\n",
    "    if l.startswith(\".I\"):\n",
    "        qry_id = l.split(\" \")[1].strip()\n",
    "    elif l.startswith(\".W\"):\n",
    "        qry_set[qry_id] = l.strip()[3:]\n",
    "        qry_id = \"\"\n",
    "    \n",
    "# Print something to see the dictionary structure, etc.\n",
    "print(f\"Number of queries = {len(qry_set)}\" + \".\\n\")\n",
    "print(\"Query # 2 : \", qry_set[\"2\"]) # note that the dictionary indexes are strings, not numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CISI Ground Truth File and placing each in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of mappings = 76.\n",
      "\n",
      "Result for Rel#7 : ['194', '263', '374', '442', '668', '835', '1180', '1428']\n"
     ]
    }
   ],
   "source": [
    "rel_set = {}\n",
    "with open('IR Project/datasets.REL') as f:\n",
    "    for l in f.readlines():\n",
    "        qry_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0]\n",
    "        doc_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1]\n",
    "        if qry_id in rel_set:\n",
    "            rel_set[qry_id].append(doc_id)\n",
    "        else:\n",
    "            rel_set[qry_id] = []\n",
    "            rel_set[qry_id].append(doc_id)\n",
    "    \n",
    "# Print something to see the dictionary structure, etc.\n",
    "print(f\"\\nNumber of mappings = {len(rel_set)}\" + \".\\n\")\n",
    "print(\"Result for Rel#7 :\" , rel_set[\"7\"]) # note that the dictionary indexes are strings, not numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Process of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    #data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    #data = convert_numbers(data)\n",
    "    data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "processed_set={}\n",
    "proc_token_id=\"\"\n",
    "proc_token_text=\"\"\n",
    "\n",
    "for i in doc_set:\n",
    "    doc_token_id=i\n",
    "    processed_set[doc_token_id]=preprocess(doc_set[str(i)])\n",
    "print(\"done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' use made technic librari slater report analysi 6300 act use 104 technic librari unit kingdom librari use one aspect wider pattern inform use inform transfer librari restrict use document take account document use outsid librari still less inform transfer oral person person librari act channel proport situat inform transfer take technic inform transfer whole doubt proport major one user technic inform particularli technolog rather scienc visit librari rare reli desk collect handbook current period person contact colleagu peopl organ even regular librari user also receiv inform way'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_set[\"2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting prcessed text to tokens and placing in a dictionary where keys are the docs id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "tokens_set={}\n",
    "doc_token_id=\"\"\n",
    "doct_token_text=\"\"\n",
    "\n",
    "for i in processed_set:\n",
    "    doc_token_id=i\n",
    "    tokens_set[doc_token_id]=word_tokenize(processed_set[str(i)])\n",
    "print(\"done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = {}\n",
    "\n",
    "for i in range(len(tokens_set)):\n",
    "    tokens = tokens_set[str(i+1)]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'18': 9,\n",
       " 'edit': 44,\n",
       " 'dewey': 13,\n",
       " 'decim': 16,\n",
       " 'classif': 105,\n",
       " 'comaromi': 1,\n",
       " 'present': 318,\n",
       " 'studi': 362,\n",
       " 'histori': 52,\n",
       " 'first': 175,\n",
       " 'ddc': 5,\n",
       " 'publish': 122,\n",
       " '1876': 4,\n",
       " 'eighteenth': 1,\n",
       " '1971': 27,\n",
       " 'futur': 95,\n",
       " 'continu': 68,\n",
       " 'appear': 86,\n",
       " 'need': 251,\n",
       " 'spite': 7,\n",
       " 'long': 48,\n",
       " 'healthi': 1,\n",
       " 'life': 39,\n",
       " 'howev': 111,\n",
       " 'full': 41,\n",
       " 'stori': 4,\n",
       " 'never': 15,\n",
       " 'told': 4,\n",
       " 'biographi': 3,\n",
       " 'briefli': 34,\n",
       " 'describ': 271,\n",
       " 'system': 515,\n",
       " 'attempt': 125,\n",
       " 'provid': 251,\n",
       " 'detail': 101,\n",
       " 'work': 252,\n",
       " 'spur': 3,\n",
       " 'growth': 67,\n",
       " 'librarianship': 49,\n",
       " 'countri': 46,\n",
       " 'abroad': 5,\n",
       " 'use': 659,\n",
       " 'made': 211,\n",
       " 'technic': 130,\n",
       " 'librari': 555,\n",
       " 'slater': 5,\n",
       " 'report': 185,\n",
       " 'analysi': 226,\n",
       " '6300': 1,\n",
       " 'act': 27,\n",
       " '104': 1,\n",
       " 'unit': 94,\n",
       " 'kingdom': 10,\n",
       " 'one': 341,\n",
       " 'aspect': 103,\n",
       " 'wider': 17,\n",
       " 'pattern': 85,\n",
       " 'inform': 660,\n",
       " 'transfer': 30,\n",
       " 'restrict': 25,\n",
       " 'document': 251,\n",
       " 'take': 66,\n",
       " 'account': 66,\n",
       " 'outsid': 19,\n",
       " 'still': 32,\n",
       " 'less': 66,\n",
       " 'oral': 4,\n",
       " 'person': 68,\n",
       " 'channel': 19,\n",
       " 'proport': 30,\n",
       " 'situat': 64,\n",
       " 'whole': 44,\n",
       " 'doubt': 12,\n",
       " 'major': 143,\n",
       " 'user': 235,\n",
       " 'particularli': 46,\n",
       " 'technolog': 110,\n",
       " 'rather': 77,\n",
       " 'scienc': 287,\n",
       " 'visit': 8,\n",
       " 'rare': 15,\n",
       " 'reli': 8,\n",
       " 'desk': 5,\n",
       " 'collect': 189,\n",
       " 'handbook': 10,\n",
       " 'current': 150,\n",
       " 'period': 112,\n",
       " 'contact': 10,\n",
       " 'colleagu': 12,\n",
       " 'peopl': 54,\n",
       " 'organ': 171,\n",
       " 'even': 89,\n",
       " 'regular': 25,\n",
       " 'also': 204,\n",
       " 'receiv': 49,\n",
       " 'way': 141,\n",
       " 'two': 232,\n",
       " 'kind': 66,\n",
       " 'power': 31,\n",
       " 'essay': 21,\n",
       " 'bibliograph': 149,\n",
       " 'control': 91,\n",
       " 'wilson': 4,\n",
       " 'relationship': 83,\n",
       " 'write': 34,\n",
       " 'knowledg': 101,\n",
       " 'inevit': 11,\n",
       " 'enter': 14,\n",
       " 'contain': 67,\n",
       " 'along': 29,\n",
       " 'much': 84,\n",
       " 'el': 6,\n",
       " 'great': 66,\n",
       " 'deal': 84,\n",
       " 'mankind': 4,\n",
       " 'stock': 12,\n",
       " 'form': 164,\n",
       " 'familiar': 15,\n",
       " 'slogan': 1,\n",
       " 'claim': 21,\n",
       " 'certain': 82,\n",
       " 'sen': 40,\n",
       " 'obtain': 94,\n",
       " 'record': 98,\n",
       " 'written': 48,\n",
       " 'simpli': 15,\n",
       " 'simpl': 50,\n",
       " 'storeh': 2,\n",
       " 'satisfactorili': 6,\n",
       " 'discuss': 262,\n",
       " 'univ': 179,\n",
       " 'final': 60,\n",
       " 'research': 362,\n",
       " 'project': 102,\n",
       " 'buckland': 6,\n",
       " 'establish': 106,\n",
       " 'nine': 28,\n",
       " 'new': 238,\n",
       " '1960': 17,\n",
       " 'provok': 3,\n",
       " 'highli': 40,\n",
       " 'stimul': 8,\n",
       " 'examin': 117,\n",
       " 'natur': 124,\n",
       " 'purpo': 142,\n",
       " 'manag': 96,\n",
       " 'academ': 88,\n",
       " 'attitud': 28,\n",
       " 'method': 266,\n",
       " 'question': 121,\n",
       " 'although': 58,\n",
       " 'chang': 117,\n",
       " 'basic': 130,\n",
       " 'difficulti': 35,\n",
       " 'remain': 38,\n",
       " 'lack': 28,\n",
       " 'object': 104,\n",
       " 'best': 46,\n",
       " 'servic': 267,\n",
       " 'ugc': 2,\n",
       " 'committ': 30,\n",
       " 'parri': 2,\n",
       " 'repot': 2,\n",
       " '267': 3,\n",
       " 'gener': 330,\n",
       " 'endor': 2,\n",
       " 'stress': 19,\n",
       " 'provi': 18,\n",
       " 'game': 4,\n",
       " 'brophi': 2,\n",
       " 'profess': 100,\n",
       " 'educ': 92,\n",
       " 'becom': 91,\n",
       " 'widespread': 12,\n",
       " 'last': 57,\n",
       " 'decad': 36,\n",
       " 'number': 206,\n",
       " 'field': 193,\n",
       " 'mani': 195,\n",
       " 'hundr': 24,\n",
       " 'year': 225,\n",
       " 'origin': 82,\n",
       " 'trace': 14,\n",
       " 'war': 13,\n",
       " 'militari': 8,\n",
       " 'train': 39,\n",
       " 'real': 27,\n",
       " 'thing': 16,\n",
       " 'either': 62,\n",
       " 'unavail': 4,\n",
       " 'danger': 8,\n",
       " 'recent': 93,\n",
       " 'time': 220,\n",
       " 'sophist': 14,\n",
       " 'larg': 188,\n",
       " 'electron': 28,\n",
       " 'comput': 276,\n",
       " 'handl': 55,\n",
       " 'complex': 54,\n",
       " 'calcul': 21,\n",
       " 'involv': 80,\n",
       " 'sinc': 99,\n",
       " '1956': 7,\n",
       " 'well': 150,\n",
       " 'develop': 378,\n",
       " 'introduc': 37,\n",
       " 'techniqu': 162,\n",
       " 'spread': 11,\n",
       " 'rapidli': 32,\n",
       " 'wide': 60,\n",
       " 'varieti': 53,\n",
       " 'disciplin': 60,\n",
       " 'today': 34,\n",
       " 'level': 72,\n",
       " 'primari': 43,\n",
       " 'school': 70,\n",
       " 'class': 51,\n",
       " 'cour': 61,\n",
       " 'experienc': 14,\n",
       " 'men': 17,\n",
       " 'women': 5,\n",
       " 'main': 59,\n",
       " 'cau': 27,\n",
       " 'explo': 13,\n",
       " 'rapid': 45,\n",
       " 'simul': 18,\n",
       " 'mathemat': 65,\n",
       " 'model': 103,\n",
       " 'possibl': 176,\n",
       " 'advanc': 69,\n",
       " 'abstract': 118,\n",
       " 'concept': 155,\n",
       " 'borko': 10,\n",
       " 'graduat': 29,\n",
       " 'includ': 168,\n",
       " 'materi': 119,\n",
       " 'characteristc': 1,\n",
       " 'type': 141,\n",
       " 'histor': 29,\n",
       " 'public': 186,\n",
       " 'industri': 86,\n",
       " 'especi': 34,\n",
       " 'state': 142,\n",
       " 'standard': 88,\n",
       " 'prepar': 75,\n",
       " 'evalu': 174,\n",
       " 'product': 107,\n",
       " 'topic': 41,\n",
       " 'call': 69,\n",
       " 'text': 88,\n",
       " 'section': 28,\n",
       " 'instruct': 23,\n",
       " 'variou': 136,\n",
       " 'supplement': 15,\n",
       " 'exampl': 103,\n",
       " 'exerci': 11,\n",
       " 'appendix': 7,\n",
       " 'brief': 21,\n",
       " 'index': 254,\n",
       " 'autom': 53,\n",
       " 'treat': 31,\n",
       " 'exten': 36,\n",
       " 'believ': 32,\n",
       " 'deserv': 6,\n",
       " 'greater': 34,\n",
       " 'emphasi': 46,\n",
       " 'past': 68,\n",
       " 'increasingli': 30,\n",
       " 'import': 153,\n",
       " 'effort': 89,\n",
       " 'expend': 9,\n",
       " 'extract': 17,\n",
       " 'student': 71,\n",
       " 'librarian': 162,\n",
       " 'abstractor': 3,\n",
       " 'benefit': 30,\n",
       " 'know': 26,\n",
       " 'understand': 58,\n",
       " 'program': 175,\n",
       " 'analyz': 94,\n",
       " 'select': 178,\n",
       " 'key': 37,\n",
       " 'sentenc': 14,\n",
       " 'segment': 12,\n",
       " 'opportun': 17,\n",
       " 'avail': 123,\n",
       " 'part': 126,\n",
       " 'volunt': 2,\n",
       " 'worker': 23,\n",
       " 'find': 115,\n",
       " 'activ': 124,\n",
       " 'pleasant': 1,\n",
       " 'reward': 16,\n",
       " 'contribut': 60,\n",
       " 'effect': 187,\n",
       " 'store': 31,\n",
       " 'chapter': 51,\n",
       " 'devot': 33,\n",
       " 'career': 13,\n",
       " 'build': 38,\n",
       " 'guid': 45,\n",
       " 'architectur': 3,\n",
       " 'issu': 50,\n",
       " 'solut': 55,\n",
       " 'ellsworth': 4,\n",
       " 'book': 295,\n",
       " 'repr': 94,\n",
       " 'success': 55,\n",
       " 'problem': 313,\n",
       " 'architect': 3,\n",
       " 'face': 34,\n",
       " 'plan': 100,\n",
       " 'colleg': 53,\n",
       " 'remodel': 1,\n",
       " 'enlarg': 4,\n",
       " 'exist': 98,\n",
       " 'structur': 173,\n",
       " 'make': 167,\n",
       " 'case': 80,\n",
       " 'done': 27,\n",
       " 'mason': 4,\n",
       " 'brown': 8,\n",
       " 'yale': 12,\n",
       " 'unsuccess': 1,\n",
       " 'except': 33,\n",
       " 'show': 97,\n",
       " 'avoid': 17,\n",
       " 'mistak': 1,\n",
       " 'identifi': 89,\n",
       " 'honor': 3,\n",
       " 'guy': 1,\n",
       " 'lyle': 4,\n",
       " 'farber': 1,\n",
       " 'staff': 50,\n",
       " 'member': 53,\n",
       " 'individu': 91,\n",
       " 'apprenticeship': 1,\n",
       " 'administr': 60,\n",
       " 'perhap': 32,\n",
       " 'signif': 95,\n",
       " 'acquir': 23,\n",
       " 'engend': 1,\n",
       " 'insist': 3,\n",
       " 'must': 90,\n",
       " 'interest': 134,\n",
       " 'content': 74,\n",
       " 'dealt': 13,\n",
       " 'love': 1,\n",
       " 'literatur': 221,\n",
       " 'respect': 58,\n",
       " 'scholarship': 8,\n",
       " 'admir': 4,\n",
       " 'good': 47,\n",
       " 'read': 45,\n",
       " 'manifest': 9,\n",
       " 'notabl': 7,\n",
       " 'admonit': 1,\n",
       " 'though': 31,\n",
       " 'primarili': 44,\n",
       " 'constantli': 7,\n",
       " 'keep': 27,\n",
       " 'mind': 22,\n",
       " 'oblig': 7,\n",
       " 'contemporari': 17,\n",
       " 'poetri': 2,\n",
       " 'fiction': 1,\n",
       " 'bell': 4,\n",
       " 'letter': 15,\n",
       " 'felt': 16,\n",
       " 'respon': 63,\n",
       " 'cross': 17,\n",
       " 'disciplinari': 7,\n",
       " 'line': 119,\n",
       " 'fell': 2,\n",
       " 'faculti': 23,\n",
       " 'mostli': 6,\n",
       " 'concern': 159,\n",
       " 'apt': 4,\n",
       " 'overlook': 7,\n",
       " 'portion': 11,\n",
       " 'substitut': 13,\n",
       " 'thorough': 9,\n",
       " 'acquaint': 5,\n",
       " 'critic': 65,\n",
       " 'review': 111,\n",
       " 'counsel': 2,\n",
       " 'presid': 5,\n",
       " 'professor': 9,\n",
       " 'thrust': 2,\n",
       " 'world': 72,\n",
       " 'impress': 6,\n",
       " 'upon': 67,\n",
       " 'us': 43,\n",
       " 'access': 80,\n",
       " 'hyman': 1,\n",
       " 'assum': 40,\n",
       " 'addit': 79,\n",
       " 'held': 20,\n",
       " 'promi': 16,\n",
       " 'analyt': 19,\n",
       " 'consid': 223,\n",
       " 'approach': 138,\n",
       " 'survey': 121,\n",
       " 'compar': 126,\n",
       " 'tradit': 55,\n",
       " 'idea': 59,\n",
       " 'direct': 66,\n",
       " 'princip': 30,\n",
       " 'data': 304,\n",
       " 'gather': 32,\n",
       " 'instrument': 20,\n",
       " 'documentari': 7,\n",
       " 'opinion': 26,\n",
       " 'questionnair': 35,\n",
       " 'follow': 102,\n",
       " '1890': 1,\n",
       " '1970': 32,\n",
       " 'shelf': 12,\n",
       " 'brow': 6,\n",
       " 'left': 14,\n",
       " 'unresolv': 2,\n",
       " 'evid': 39,\n",
       " 'resist': 5,\n",
       " 'exhaust': 16,\n",
       " 'confirm': 7,\n",
       " 'open': 14,\n",
       " 'rel': 89,\n",
       " 'locat': 32,\n",
       " 'meant': 5,\n",
       " 'arou': 3,\n",
       " 'intellectu': 27,\n",
       " 'social': 128,\n",
       " 'polit': 15,\n",
       " 'averag': 37,\n",
       " 'citizen': 7,\n",
       " 'affect': 48,\n",
       " 'democrat': 3,\n",
       " 'self': 24,\n",
       " 'realiz': 14,\n",
       " 'definit': 54,\n",
       " 'vari': 31,\n",
       " 'greatli': 22,\n",
       " 'indulg': 2,\n",
       " 'untutor': 1,\n",
       " 'benefici': 5,\n",
       " 'reader': 95,\n",
       " 'valuabl': 23,\n",
       " 'guidanc': 8,\n",
       " 'scholar': 16,\n",
       " 'resourc': 70,\n",
       " 'palmour': 3,\n",
       " 'recommend': 43,\n",
       " 'nation': 131,\n",
       " 'improv': 115,\n",
       " '48': 4,\n",
       " 'percent': 26,\n",
       " 'interlibrari': 10,\n",
       " 'loan': 15,\n",
       " 'bulk': 3,\n",
       " 'satisfi': 31,\n",
       " 'photocopi': 10,\n",
       " 'rang': 42,\n",
       " 'augment': 8,\n",
       " 'request': 70,\n",
       " 'focu': 36,\n",
       " 'physic': 75,\n",
       " 'base': 286,\n",
       " 'commun': 194,\n",
       " 'design': 194,\n",
       " 'featur': 43,\n",
       " 'without': 68,\n",
       " 'initi': 43,\n",
       " 'confin': 4,\n",
       " 'depend': 46,\n",
       " 'deliveri': 10,\n",
       " 'journal': 143,\n",
       " 'articl': 131,\n",
       " 'center': 86,\n",
       " 'comprehen': 43,\n",
       " 'subject': 235,\n",
       " 'coverag': 32,\n",
       " 'exclud': 11,\n",
       " 'medicin': 29,\n",
       " 'worthwhil': 4,\n",
       " 'irrespect': 3,\n",
       " 'languag': 123,\n",
       " 'acquisit': 40,\n",
       " 'ford': 5,\n",
       " 'scope': 36,\n",
       " 'outlin': 52,\n",
       " 'introduct': 35,\n",
       " 'acknowledg': 7,\n",
       " 'polici': 46,\n",
       " 'serial': 48,\n",
       " 'kindr': 1,\n",
       " 'relat': 226,\n",
       " 'thoroughli': 7,\n",
       " 'paper': 268,\n",
       " 'cite': 46,\n",
       " 'refer': 165,\n",
       " 'note': 37,\n",
       " 'central': 44,\n",
       " 'order': 115,\n",
       " 'routin': 9,\n",
       " 'manual': 45,\n",
       " 'practic': 160,\n",
       " 'treatment': 21,\n",
       " 'particular': 113,\n",
       " 'depth': 20,\n",
       " 'modest': 10,\n",
       " 'enorm': 11,\n",
       " '3rd': 2,\n",
       " 'clark': 2,\n",
       " 'ligu': 1,\n",
       " 'de': 11,\n",
       " 'bibliothequ': 1,\n",
       " 'europeenn': 1,\n",
       " 'recherch': 1,\n",
       " 'liber': 4,\n",
       " 'set': 115,\n",
       " 'intern': 87,\n",
       " 'non': 30,\n",
       " 'govern': 62,\n",
       " 'aim': 54,\n",
       " 'close': 40,\n",
       " 'collabor': 14,\n",
       " 'western': 9,\n",
       " 'europ': 5,\n",
       " 'help': 73,\n",
       " 'qualiti': 42,\n",
       " 'second': 60,\n",
       " 'meet': 59,\n",
       " 'assembl': 10,\n",
       " 'luxembourg': 1,\n",
       " '1972': 17,\n",
       " 'decid': 25,\n",
       " 'hold': 38,\n",
       " 'seminar': 9,\n",
       " 'third': 37,\n",
       " 'charg': 25,\n",
       " 'would': 123,\n",
       " 'european': 7,\n",
       " 'lend': 15,\n",
       " 'feasibl': 35,\n",
       " 'centr': 10,\n",
       " 'prefer': 29,\n",
       " 'machin': 112,\n",
       " 'readabl': 39,\n",
       " 'whatev': 13,\n",
       " 'mean': 153,\n",
       " 'propo': 95,\n",
       " 'area': 126,\n",
       " 'council': 17,\n",
       " 'grant': 14,\n",
       " 'toward': 74,\n",
       " 'cost': 150,\n",
       " 'sussex': 1,\n",
       " '17': 7,\n",
       " '19': 6,\n",
       " 'septemb': 11,\n",
       " '1973': 13,\n",
       " 'ad695049': 1,\n",
       " 'wooster': 1,\n",
       " 'ever': 21,\n",
       " 'pretend': 2,\n",
       " 'expert': 19,\n",
       " 'microfich': 6,\n",
       " 'nevertheless': 11,\n",
       " 'invit': 7,\n",
       " 'address': 24,\n",
       " 'annual': 33,\n",
       " 'northeastern': 1,\n",
       " 'confer': 30,\n",
       " 'waltham': 1,\n",
       " 'massachusett': 4,\n",
       " 'april': 11,\n",
       " '1968': 37,\n",
       " 'temer': 1,\n",
       " 'like': 79,\n",
       " 'fich': 2,\n",
       " 'uniform': 9,\n",
       " 'feder': 21,\n",
       " 'cuddli': 1,\n",
       " 'revi': 22,\n",
       " 'ad': 33,\n",
       " '669204': 1,\n",
       " 'thesi': 6,\n",
       " 'rest': 5,\n",
       " 'frequent': 38,\n",
       " 'neglect': 8,\n",
       " 'principl': 74,\n",
       " 'goe': 10,\n",
       " 'beyond': 17,\n",
       " 'commonli': 12,\n",
       " 'accept': 56,\n",
       " 'function': 108,\n",
       " 'circul': 38,\n",
       " 'storag': 84,\n",
       " 'mere': 25,\n",
       " 'hou': 16,\n",
       " 'extend': 30,\n",
       " 'teach': 27,\n",
       " 'process': 216,\n",
       " 'afford': 9,\n",
       " 'encourag': 16,\n",
       " 'better': 52,\n",
       " 'essenti': 46,\n",
       " 'tool': 59,\n",
       " 'fulli': 29,\n",
       " 'effici': 81,\n",
       " 'retain': 4,\n",
       " 'substanti': 26,\n",
       " 'independ': 32,\n",
       " 'found': 94,\n",
       " 'footnot': 5,\n",
       " 'bibliographi': 52,\n",
       " 'brought': 22,\n",
       " 'date': 40,\n",
       " 'flow': 42,\n",
       " 'laboratori': 25,\n",
       " 'allen': 6,\n",
       " 'thoma': 9,\n",
       " 'cohen': 1,\n",
       " 'stephen': 9,\n",
       " 'modifi': 15,\n",
       " 'sociometr': 4,\n",
       " 'network': 63,\n",
       " 'result': 291,\n",
       " 'interact': 61,\n",
       " 'star': 1,\n",
       " 'adopt': 37,\n",
       " 'congress': 32,\n",
       " 'matthi': 1,\n",
       " 'procedur': 105,\n",
       " 'may': 187,\n",
       " 'serv': 77,\n",
       " 'exact': 8,\n",
       " 'seri': 51,\n",
       " 'suggest': 143,\n",
       " 'step': 45,\n",
       " 'proven': 6,\n",
       " 'actual': 62,\n",
       " 'necessari': 72,\n",
       " 'criteria': 47,\n",
       " 'reclassif': 2,\n",
       " 'attent': 65,\n",
       " 'given': 157,\n",
       " 'lc': 13,\n",
       " 'law': 26,\n",
       " 'pz3': 1,\n",
       " 'pz4': 1,\n",
       " 'tabl': 34,\n",
       " 'viiia': 1,\n",
       " 'ixa': 1,\n",
       " 'throughout': 28,\n",
       " 'entir': 32,\n",
       " 'explain': 36,\n",
       " 'applic': 147,\n",
       " 'illustr': 49,\n",
       " 'mechan': 71,\n",
       " 'catalogu': 38,\n",
       " 'card': 51,\n",
       " 'copi': 26,\n",
       " 'significantli': 16,\n",
       " 'xerox': 3,\n",
       " 'oper': 203,\n",
       " 'annot': 7,\n",
       " 'judg': 28,\n",
       " 'reclassifi': 1,\n",
       " 'wish': 17,\n",
       " 'delv': 1,\n",
       " 'deepli': 3,\n",
       " 'tortuou': 1,\n",
       " 'frustrat': 4,\n",
       " '50': 13,\n",
       " 'catalog': 109,\n",
       " 'enclo': 1,\n",
       " 'parenth': 1,\n",
       " 'sourc': 102,\n",
       " 'support': 72,\n",
       " 'argument': 15,\n",
       " 'adventur': 3,\n",
       " 'voigt': 1,\n",
       " 'scholarli': 12,\n",
       " 'fill': 15,\n",
       " 'progress': 66,\n",
       " 'obviou': 14,\n",
       " 'front': 5,\n",
       " 'caught': 1,\n",
       " 'enthusiast': 2,\n",
       " 'visual': 7,\n",
       " 'potenti': 64,\n",
       " 'certainli': 16,\n",
       " 'everi': 45,\n",
       " 'volum': 80,\n",
       " 'group': 133,\n",
       " 'demonstr': 37,\n",
       " 'implic': 46,\n",
       " 'inventori': 15,\n",
       " 'expect': 46,\n",
       " 'dedic': 3,\n",
       " 'realiti': 6,\n",
       " 'assess': 47,\n",
       " 'go': 27,\n",
       " 'fast': 8,\n",
       " 'hope': 52,\n",
       " 'get': 19,\n",
       " 'statu': 31,\n",
       " 'produc': 98,\n",
       " 'lie': 10,\n",
       " 'ahead': 3,\n",
       " 'age': 27,\n",
       " 'marc': 26,\n",
       " 'busi': 30,\n",
       " 'easier': 8,\n",
       " 'immedi': 26,\n",
       " 'file': 105,\n",
       " 'manipul': 19,\n",
       " 'defend': 2,\n",
       " 'adequ': 46,\n",
       " 'fact': 74,\n",
       " 'determin': 119,\n",
       " 'difficult': 31,\n",
       " 'widen': 4,\n",
       " 'dimen': 12,\n",
       " 'reflect': 48,\n",
       " 'name': 53,\n",
       " 'media': 25,\n",
       " 'togeth': 48,\n",
       " 'children': 4,\n",
       " 'clear': 27,\n",
       " 'rule': 57,\n",
       " 'bibliotherapi': 1,\n",
       " 'slow': 5,\n",
       " 'theori': 140,\n",
       " 'manageri': 8,\n",
       " 'come': 36,\n",
       " 'provoc': 2,\n",
       " 'controversi': 13,\n",
       " 'degr': 43,\n",
       " 'assist': 40,\n",
       " 'modern': 39,\n",
       " 'dissemin': 69,\n",
       " 'mauerhoff': 2,\n",
       " '1974': 5,\n",
       " 'duplic': 20,\n",
       " 'previou': 63,\n",
       " 'complement': 9,\n",
       " 'earlier': 35,\n",
       " 'gap': 13,\n",
       " 'prior': 18,\n",
       " '1966': 23,\n",
       " 'bold': 2,\n",
       " 'sdi': 38,\n",
       " 'descript': 102,\n",
       " 'luhn': 6,\n",
       " '1958': 10,\n",
       " '1961b': 1,\n",
       " 'post': 19,\n",
       " 'boom': 2,\n",
       " 'began': 18,\n",
       " 'lose': 3,\n",
       " 'ground': 9,\n",
       " 'popular': 9,\n",
       " 'therefor': 66,\n",
       " 'interpret': 41,\n",
       " 'implement': 42,\n",
       " 'evolut': 13,\n",
       " 'light': 18,\n",
       " 'perform': 137,\n",
       " 'compani': 11,\n",
       " 'agenc': 22,\n",
       " 'societi': 48,\n",
       " 'fourteen': 3,\n",
       " 'herdan': 1,\n",
       " 'tri': 36,\n",
       " 'give': 66,\n",
       " 'statist': 102,\n",
       " 'properti': 42,\n",
       " 'common': 44,\n",
       " 'thread': 6,\n",
       " 'multifari': 1,\n",
       " 'embodi': 3,\n",
       " 'scatter': 21,\n",
       " 'linguist': 27,\n",
       " 'philosoph': 13,\n",
       " 'mathematician': 7,\n",
       " 'engin': 68,\n",
       " 'idiom': 1,\n",
       " 'belong': 11,\n",
       " 'quantit': 41,\n",
       " 'peculiar': 5,\n",
       " 'convent': 37,\n",
       " 'put': 34,\n",
       " 'differ': 197,\n",
       " 'latter': 32,\n",
       " 'compri': 9,\n",
       " 'paramet': 27,\n",
       " 'appli': 104,\n",
       " 'econom': 73,\n",
       " 'demographi': 1,\n",
       " 'former': 17,\n",
       " 'characteri': 2,\n",
       " 'adumbr': 1,\n",
       " 'author': 155,\n",
       " 'choic': 32,\n",
       " 'chanc': 7,\n",
       " 'taken': 47,\n",
       " 'shape': 10,\n",
       " 'shall': 18,\n",
       " 'foundat': 19,\n",
       " 'laid': 5,\n",
       " 'truli': 3,\n",
       " 'sensibl': 4,\n",
       " 'langu': 1,\n",
       " 'parol': 1,\n",
       " 'dichotomi': 1,\n",
       " 'sampl': 57,\n",
       " 'jewett': 1,\n",
       " 'charl': 7,\n",
       " 'coffin': 1,\n",
       " 'american': 73,\n",
       " '1841': 1,\n",
       " '1868': 2,\n",
       " 'harri': 6,\n",
       " 'mark': 16,\n",
       " 'begin': 37,\n",
       " 'associ': 105,\n",
       " 'scene': 7,\n",
       " 'dynam': 19,\n",
       " 'figur': 34,\n",
       " 'melvil': 1,\n",
       " 'ammi': 1,\n",
       " 'cutter': 5,\n",
       " 'extrem': 9,\n",
       " 'era': 6,\n",
       " 'quarter': 13,\n",
       " 'centuri': 29,\n",
       " 'prece': 6,\n",
       " 'philadelphia': 4,\n",
       " 'charact': 57,\n",
       " 'influenc': 52,\n",
       " 'america': 7,\n",
       " 'ignor': 12,\n",
       " 'nineteenth': 3,\n",
       " 'risk': 3,\n",
       " 'misinterpret': 1,\n",
       " 'pivot': 2,\n",
       " 'reapprai': 3,\n",
       " 'seem': 73,\n",
       " 'appropri': 56,\n",
       " 'aggress': 2,\n",
       " 'lorenz': 2,\n",
       " 'vertebr': 1,\n",
       " 'point': 113,\n",
       " 'total': 71,\n",
       " 'predat': 1,\n",
       " 'biolog': 28,\n",
       " 'necess': 13,\n",
       " 'defenc': 1,\n",
       " 'territori': 2,\n",
       " 'corner': 2,\n",
       " 'anim': 4,\n",
       " 'mix': 2,\n",
       " 'innat': 2,\n",
       " 'drive': 4,\n",
       " 'thu': 61,\n",
       " 'lead': 45,\n",
       " 'reduct': 17,\n",
       " 'intraspecif': 1,\n",
       " 'damag': 1,\n",
       " 'fiercer': 1,\n",
       " 'wolv': 1,\n",
       " 'escap': 1,\n",
       " 'pack': 2,\n",
       " 'virtual': 10,\n",
       " 'imposs': 4,\n",
       " 'co': 38,\n",
       " 'fight': 3,\n",
       " 'surviv': 10,\n",
       " 'wherea': 11,\n",
       " 'proverbi': 1,\n",
       " 'peac': 3,\n",
       " 'dove': 1,\n",
       " 'prevent': 14,\n",
       " 'violent': 1,\n",
       " 'often': 59,\n",
       " 'fatal': 1,\n",
       " 'attack': 4,\n",
       " 'weaker': 2,\n",
       " 'mate': 1,\n",
       " 'human': 65,\n",
       " 'speci': 7,\n",
       " 'cultur': 23,\n",
       " 'darwinian': 1,\n",
       " 'strict': 4,\n",
       " 'canal': 1,\n",
       " 'man': 33,\n",
       " 'innum': 3,\n",
       " 'adapt': 22,\n",
       " 'ritual': 2,\n",
       " 'behavior': 52,\n",
       " 'analog': 11,\n",
       " 'homolog': 1,\n",
       " 'event': 12,\n",
       " 'fascin': 2,\n",
       " 'master': 18,\n",
       " 'manpow': 13,\n",
       " 'asheim': 2,\n",
       " 'offici': 9,\n",
       " 'statement': 33,\n",
       " 'offic': 34,\n",
       " 'explor': 45,\n",
       " 'eventu': 14,\n",
       " 'assumpt': 24,\n",
       " 'occup': 4,\n",
       " 'broader': 11,\n",
       " 'supervi': 4,\n",
       " 'requir': 151,\n",
       " 'complet': 78,\n",
       " 'encompass': 5,\n",
       " 'engag': 19,\n",
       " 'mainten': 17,\n",
       " 'norm': 7,\n",
       " 'defin': 97,\n",
       " 'pre': 13,\n",
       " 'posit': 47,\n",
       " 'pilot': 17,\n",
       " 'braden': 1,\n",
       " 'undertaken': 26,\n",
       " 'high': 54,\n",
       " 'uncertainti': 7,\n",
       " 'among': 99,\n",
       " 'regard': 48,\n",
       " 'extent': 29,\n",
       " 'miss': 12,\n",
       " 'gave': 8,\n",
       " 'ohio': 9,\n",
       " 'concret': 4,\n",
       " 'indic': 119,\n",
       " 'loss': 15,\n",
       " 'greatest': 19,\n",
       " 'convey': 9,\n",
       " 'voic': 5,\n",
       " 'complaint': 1,\n",
       " 'tend': 24,\n",
       " 'forc': 34,\n",
       " 'cuadra': 9,\n",
       " 'think': 28,\n",
       " 'fores': 2,\n",
       " 'move': 11,\n",
       " '2000': 5,\n",
       " 'anyth': 4,\n",
       " 'tomorrow': 2,\n",
       " 'algebra': 6,\n",
       " 'maltsev': 1,\n",
       " 'far': 52,\n",
       " 'back': 18,\n",
       " '1920': 2,\n",
       " 'forti': 10,\n",
       " 'overwhelm': 7,\n",
       " 'algebraist': 1,\n",
       " 'investig': 120,\n",
       " 'ring': 12,\n",
       " 'lattic': 1,\n",
       " 'theoret': 64,\n",
       " 'arbitrari': 7,\n",
       " 'due': 18,\n",
       " 'birkhoff': 1,\n",
       " '1935': 2,\n",
       " 'tarski': 1,\n",
       " 'formul': 43,\n",
       " 'equip': 33,\n",
       " 'contrast': 18,\n",
       " 'abund': 4,\n",
       " 'apparatu': 3,\n",
       " 'logic': 50,\n",
       " 'fruit': 8,\n",
       " 'classic': 7,\n",
       " 'discov': 23,\n",
       " '1936': 2,\n",
       " 'next': 15,\n",
       " 'twenti': 22,\n",
       " 'five': 52,\n",
       " 'gradual': 11,\n",
       " 'becam': 20,\n",
       " 'intim': 4,\n",
       " 'despit': 10,\n",
       " 'speak': 12,\n",
       " 'singl': 57,\n",
       " 'formal': 57,\n",
       " 'predic': 4,\n",
       " 'calculu': 2,\n",
       " 'border': 3,\n",
       " 'doyl': 4,\n",
       " 'exploit': 16,\n",
       " 'word': 99,\n",
       " 'occurr': 13,\n",
       " 'retriev': 296,\n",
       " 'mental': 7,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_vocab_size = len(DF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7132"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab = [x for x in DF]\n",
    "N=len(total_vocab)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf done\n"
     ]
    }
   ],
   "source": [
    "doc = 0\n",
    "N=len(tokens_set)\n",
    "tf_idf = {}\n",
    "\n",
    "for i in range(len(tokens_set)):\n",
    "    if(i>0):\n",
    "        tokens = tokens_set[str(i)]\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf[doc,token] = tf*idf\n",
    "    doc += 1\n",
    "\n",
    "print(\"tf-idf done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching scores done\n"
     ]
    }
   ],
   "source": [
    "def matching_score(query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    \n",
    "    query_weights = {}\n",
    "\n",
    "    for key in tf_idf:\n",
    "        \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for i in query_weights[:10]:\n",
    "        l.append(i[0])\n",
    "    \n",
    "    print(l)\n",
    "    \n",
    "\n",
    "print(\"Matching scores done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Score\n",
      "\n",
      "Query: What is information science?  Give definitions where possible.\n",
      "\n",
      "['inform', 'scienc', 'give', 'definit', 'possibl']\n",
      "\n",
      "[469, 1181, 1179, 1133, 1142, 599, 60, 372, 540, 1087]\n"
     ]
    }
   ],
   "source": [
    "matching_score(qry_set[\"3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosine similarilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros((N, total_vocab_size))   #total_vocab_size is the length of DF\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vector(tokens):\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = math.log((N+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(k, query):\n",
    "    \n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    \n",
    "    #print(\"\\nQuery:\", query)\n",
    "    \n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = gen_vector(tokens)\n",
    "    \n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "    \n",
    "    \n",
    "    #print(\"Most similar Dpocuments-IDs : \")\n",
    "    \n",
    "    #print(out)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 469,  445, 1181, 1179,  540, 1142, 1133,   85, 1235, 1077,  599,\n",
       "        803,  914, 1030,  177,  456,  553,   42,  372, 1137], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = cosine_similarity(20,qry_set[\"3\"])\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading ground truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60, 85, 118, 123, 126, 131, 133, 137, 138, 160, 346, 359, 363, 372, 412, 445, 454, 461, 463, 469, 532, 539, 540, 553, 554, 555, 585, 590, 599, 640, 660, 664, 803, 901, 909, 911, 1030, 1053, 1077, 1179, 1181, 1190, 1191, 1326]\n"
     ]
    }
   ],
   "source": [
    "rel_set = {}\n",
    "with open('IR Project/Datasets.REL') as f:\n",
    "    for l in f.readlines():\n",
    "        qry_id = l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0]\n",
    "        doc_id = int(l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1])\n",
    "        if qry_id in rel_set:\n",
    "            rel_set[qry_id].append(doc_id)\n",
    "        else:\n",
    "            rel_set[qry_id] = []\n",
    "            rel_set[qry_id].append(doc_id) \n",
    "    \n",
    "    \n",
    "print(rel_set[\"3\"]) # note that the dictionary indexes are strings, not numbers. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list=[]\n",
    "recall_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list=[]\n",
    "recall_list=[]\n",
    "\n",
    "for i in range(1,len(doc_set)):\n",
    "    try:\n",
    "        result_from_cosine=cosine_similarity(6 , qry_set[str(i)]).tolist()\n",
    "        result_from_ground_truth=rel_set[str(i)]\n",
    "        \n",
    "        true_Positive=len(set(result_from_cosine) & set(result_from_ground_truth)) #set(a) & set(b) gives us intersection between a and b\n",
    "        false_Positive=len(np.setdiff1d(result_from_cosine , result_from_ground_truth))\n",
    "        false_Negative=len(np.setdiff1d(result_from_ground_truth , result_from_cosine))\n",
    "        #print(\"true psotive\",true_Positive)\n",
    "        #print(\"false negative\",false_Negative)\n",
    "        \n",
    "        try:\n",
    "            precission= (true_Positive) / ( true_Positive + false_Positive )\n",
    "            recall= (true_Positive) / (true_Positive + false_Negative)\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "\n",
    "        precision_list.append(precission)\n",
    "        recall_list.append(recall)\n",
    "        \n",
    "        \n",
    "        \n",
    "    except KeyError:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.99999999999999"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(precision_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.822298142574425"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recall_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter a Query, Get your Result \n",
    "### Simple User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query here : 'What is information science?  Give definitions where possible.'\n",
      "\n",
      "\n",
      "Entered Query is :  'What is information science?  Give definitions where possible.'\n",
      "\n",
      "\n",
      "Related Documents IDs are :  [ 469  445 1181 1179  540 1142 1133   85 1235 1077]\n",
      "\n",
      "Do you want to retrive the document ? \n",
      " press Y to see all related docs \n",
      " Press S to see a single document with given id \n",
      " Press N to exit \n",
      "Y\n",
      "\n",
      "\n",
      "*** You are in All Document Retriveal Mood ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc-Id : 469 \n",
      "\t The Phenomena of Interest to Information Science Wersig, Gernot Neveling, Ulrich Discusses the various explicit and implicit definitions of information and information science, against a view of their historical development.. Shows how the various views of information science overlap with other disciplines, and concludes with a proposal for a definition of information science based on social need.. A schema of information sciences is put forward with the plea that any discussion of information and information science should first declare the definitions to be used.. \n",
      "\n",
      "\n",
      "Doc-Id : 445 \n",
      "\t A Definition of Relevance for Information Retrieval Cooper, W.S. The concept of \"relevance\", sometimes also called \"pertinence\" or \"aboutness\", is central to the theory of information retrieval.. Unfortunately, however, there is at present no consensus as to how this notion should be defined.. The purpose of this paper is to propose and defend a definition of what it means to say that a piece of stored information is \"relevant\" to the information need of a retrieval system user.. The suggested definition explicates relevance in terms of logical implication.. For one yes-or-no question answering system which operates with one of the standard formalized languages, the definition provides a mathematically precise criterion of relevance.. For other types of fact retrieval systems and reference retrieval systems, including all systems whose stored information is expressed in natural language, the definition is not mathematically precise but is nevertheless still helpful on a conceptual level.. \n",
      "\n",
      "\n",
      "Doc-Id : 1181 \n",
      "\t The Origins of the Information Crisis:  A Contribution to the Statement of the Problem Yurko, A. A. The different explanations of the nature of the information problems now facing science and their causes are cited and shown to be debatable.. It is necessary to give a definition of \"information crisis\", this widely used concept in informatics and the science of science.. The author suggests one such definition, which reflects the specific historical nature of the possible manifestations of the crisis.. The \"cumulativistic\" concept of the progress of science is criticized as it rules out the possibility of finding the true causes of the information crisis.. The major cause is asserted to lie with the nature of the contemporary social production.. An approach to studying into the origins of the problem is suggested.. \n",
      "\n",
      "\n",
      "Doc-Id : 1179 \n",
      "\t Topical Aspects of Informatics to-date Kozachkov, L. S. A definition of informatics is given, its method and subject are discussed, and the aims and prospects of the science are outlined.. The author holds it to be an important achievement of the research in the fields of informatics and the science of science in the past few years that information flows have come to be viewed as system with definite and understandable regularities, which should be taken into consideration when working out information retrieval system.. \n",
      "\n",
      "\n",
      "Doc-Id : 540 \n",
      "\t Information:  Methodology Ursul, A.D. This book sheds light on basic problems, principles and results of philosophical-methodological research in information concepts, gives critical analysis of its idealistic interpretation. Author proves possibility more general definition of information using categories of reflectivity and inequality. Both mathematical variants (statistical-probability and nonstatistical) as well as semantic concepts of information are analyzed, basic information species and functions in human society are determined. \n",
      "\n",
      "\n",
      "Doc-Id : 1142 \n",
      "\t Science on science - Introduction to a general science of science Dobrov, G.M. This book generalizes world and soviet experience of science, gives original representation of science as informational process which allows one to use quantitative methods in scientometrics, analyzes extensive data on the experience of formulating scientific potential and organizing scientific work, formulates general principles of organization, management and disposition of modern scientific centers. In particular methodological problems of planning and prediction of science are examined. \n",
      "\n",
      "\n",
      "Doc-Id : 1133 \n",
      "\t Thesauri in Informatics and in Theoretical Semantics Shreider, Yu. A. The possibilities are discussed of a universal definition of the concept of \"thesaurus\"; thesaurus structures and construction methods are considered.. \n",
      "\n",
      "\n",
      "Doc-Id : 85 \n",
      "\t Information Science: Toward the Development of a True Scientific Discipline Yovits, M. C. It is pointed out that if information science is to be considered a \"true\" science similar to physics or chemistry then it must have a set of concepts and analytical expression which apply to the flow of information in a general way.. In several previous papers, the author and a colleague have described a model of a generalized information system which has  wide, and perhaps universal applicability.. This paper elaborates on this model and indicates the range of its applicability.. Several fundamental quantities are defined specifically in a way which allows for quantification.. It is pointed out in this paper that this model can be the basis for the development of a \"true\" science of information with all of the necessary requirements for a science.. By the use of this model and the definition of a \"true\" science, the goals and requirements for a curriculum in information science are thus established.. Within this context, information is defined as data of value in decision making.. Quantitative measures of information can be obtained by relating information to specific observable actions which can be measured physically.. \n",
      "\n",
      "\n",
      "Doc-Id : 1235 \n",
      "\t Public Knowledge An Essay Concerning the Social Dimension of Science Ziman, J.M. Natural Science, whose internal development for three centuries is so uniform, well-documented and relatively self- generating, is an obvious candidate for such treatment.  And having noticed the intellectual connections between the ideas of various scholars, we must surely pass on to an investigation of the social relations through which those connections are established.  How do scientists teach, communicate with, promote, criticize, honour, give ear to, give patronage to, one another? What is the nature of the community to which they adhere? \n",
      "\n",
      "\n",
      "Doc-Id : 1077 \n",
      "\t Comments about Terminology in Documentation. II:  communication and Information Wersig, V.G. Meyer-Uhlenried, K. Developing from the definitions of the concept language a terminological represented model of the communication process can be deduced that shows the transfer of meaning between communicator and recipient through communication channels and mediators.  The distinguished communication structures are communication sequences, chains, nets and systems.  With an universal definition of data as fixed representations of facts by means of signs the various meanings of the term \"information\" are terminological to differentiate as \"information,\" \"information process\" and \"informations.\" The theory of signs makes the differentiation of syntactical, semantical, sigmatical and pragmatical information possible.  Adequate to communication process and system we can determine informing process and information system. \n"
     ]
    }
   ],
   "source": [
    "query=input(\"Enter your query here : \")\n",
    "\n",
    "Q=cosine_similarity(10,query)\n",
    "\n",
    "print(\"\\n\\nEntered Query is : \" , query)\n",
    "print(\"\\n\\nRelated Documents IDs are : \", Q)\n",
    "print(\"\\nDo you want to retrive the document ? \\n press Y to see all related docs \\n Press S to see a single document with given id \\n Press N to exit \")\n",
    "\n",
    "entered_option=input()\n",
    "    \n",
    "if entered_option == \"Y\":\n",
    "\n",
    "    print(\"\\n\\n*** You are in All Document Retriveal Mood ***\\n\\n\")\n",
    "\n",
    "    for i in range(len(Q)):\n",
    "            print(\"\\n\\nDoc-Id :\", Q[i] , \"\\n\\t\" ,doc_set[str(Q[i])])\n",
    "           \n",
    "elif entered_option == \"S\":\n",
    "    print(\"Enter your desired document ID : \")\n",
    "    doc_id=input()\n",
    "    print(\"Doc-Id : \", doc_id, \"\\n\\t\" ,doc_set[doc_id])\n",
    "        \n",
    "\n",
    "else:\n",
    "    print(\"Thank you for using our Information System\")\n",
    "    print(\"Hassan Ashiq & Usman Ali Abbasi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is information science?  Give definitions where possible.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry_set[\"3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## IR Semester Project\n",
    " ### Submitted to Ma'am Naima\n",
    "- #### Submitted by : Hassan Ashiq BESE 23 C , Usman Ali Abbasi 23 C\n",
    "- ###### Link to my GitHub Repository <a href=\"https://github.com/hassanashiqasse/PCA\">Click Here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
